{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiZq0X2oB5t"
      },
      "source": [
        "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
        "\n",
        "# **The Perceptron** (20 pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\entro\\\\OneDrive\\\\Documents\\\\Yahya\\\\UNT-IS PhD\\\\UNT - Courses\\\\CSCE 5218 - Deep Learning\\\\Module 04'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\entro\\OneDrive\\Documents\\Yahya\\UNT-IS PhD\\UNT - Courses\\CSCE 5218 - Deep Learning\\Module 04\n"
          ]
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "print(f\"Current working directory: {current_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File path: c:\\Users\\entro\\OneDrive\\Documents\\Yahya\\UNT-IS PhD\\UNT - Courses\\CSCE 5218 - Deep Learning\\Module 04\\train.dat\n",
            "File path: c:\\Users\\entro\\OneDrive\\Documents\\Yahya\\UNT-IS PhD\\UNT - Courses\\CSCE 5218 - Deep Learning\\Module 04\\test_small.dat\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_path = os.path.join(current_dir, \"train.dat\")\n",
        "print(f\"File path: {train_path}\")\n",
        "\n",
        "test_path = os.path.join(current_dir, \"test_small.dat\")\n",
        "print(f\"File path: {test_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\n",
            "0\t0\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
            "1\t0\t1\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t1\n",
            "0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
            "0\t1\t1\t1\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
            "1\t0\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
            "1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
            "0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t0\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "1\t0\t1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
            "1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t1\t1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t0\t1\t1\t1\n",
            "1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
            "0\t1\t0\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t0\n",
            "1\t0\t1\t0\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\n",
            "0\t0\t0\t1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t1\t0\t0\t1\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
            "0\t1\t0\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
            "1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t1\t0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\n",
            "1\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
            "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
            "1\t1\t0\t0\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t0\t0\t1\t1\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "1\t1\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t0\t0\t0\t1\t1\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
            "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t1\n",
            "1\t0\t0\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "0\t0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
            "0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
            "1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "1\t0\t1\t0\t1\t0\t0\t1\t1\t0\t0\t1\t1\t0\n",
            "1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t1\n",
            "0\t1\t0\t0\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
            "1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\n",
            "0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t0\t1\t1\t0\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "0\t1\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
            "0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
            "1\t1\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\n",
            "1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\n",
            "0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\n",
            "1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\n",
            "1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t1\n",
            "0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
            "1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
            "1\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t1\n",
            "0\t0\t0\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "1\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t1\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
            "1\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
            "1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t1\t1\t1\n",
            "1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
            "1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
            "1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t1\t1\n",
            "1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
            "1\t0\t1\t1\t1\t0\t0\t1\t0\t0\t0\t1\t1\t1\n",
            "1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\n",
            "1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
            "1\t1\t1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t0\n",
            "1\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t0\t0\t0\t1\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
            "1\t1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
            "0\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t0\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "1\t1\t1\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
            "0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t0\t0\t0\t0\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
            "1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\n",
            "0\t0\t1\t0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
            "0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
            "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "1\t1\t1\t0\t0\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t0\t0\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t1\n",
            "1\t0\t0\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\n",
            "0\t1\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
            "1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\t0\n",
            "1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\n",
            "1\t0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
            "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t1\t1\t1\t0\t1\t0\t1\t0\t1\t1\n",
            "0\t1\t0\t1\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t1\t0\n",
            "1\t1\t1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t1\t0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t1\t1\n",
            "0\t0\t0\t1\t1\t0\t1\t1\t0\t1\t1\t1\t1\t1\n",
            "1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
            "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t0\t1\t1\n",
            "1\t0\t0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\t0\t1\t1\n",
            "0\t1\t0\t1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t0\n",
            "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\n",
            "1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
            "1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t1\n",
            "0\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t0\t1\t1\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t0\n",
            "0\t0\t1\t1\t0\t0\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t1\t0\t0\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t0\t0\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
            "1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t0\t0\t1\t1\n",
            "0\t1\t1\t1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t0\t1\t1\t1\n",
            "1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\n",
            "1\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t1\t0\n",
            "0\t0\t1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "1\t0\t1\t1\t0\t0\t1\t0\t0\t1\t1\t1\t1\t0\n",
            "0\t0\t1\t1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "1\t1\t1\t1\t1\t0\t0\t1\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t1\t0\t1\t0\t0\t1\t1\t0\n",
            "0\t1\t1\t0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t0\t1\t0\t0\t1\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t1\t0\t1\t1\t0\t1\t1\t1\n",
            "1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t1\t1\t0\t0\t1\t0\t0\t0\t1\t0\t1\t0\n",
            "1\t1\t0\t1\t0\t1\t1\t0\t1\t1\t0\t1\t1\t0\n",
            "1\t0\t1\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\n",
            "1\t0\t1\t0\t0\t0\t1\t1\t0\t1\t1\t0\t1\t0\n",
            "1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t1\t0\t1\t1\n",
            "0\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
            "1\t0\t1\t0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t1\t0\t1\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
            "0\t1\t1\t0\t0\t1\t1\t1\t0\t0\t0\t1\t1\t0\n",
            "1\t0\t1\t0\t1\t1\t1\t0\t0\t0\t0\t1\t1\t0\n",
            "1\t1\t0\t1\t0\t0\t0\t0\t1\t0\t1\t1\t1\t0\n",
            "0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t1\t0\n",
            "0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t1\t1\t1\t1\t0\t1\t0\t1\t1\t0\n",
            "1\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\n",
            "0\t1\t0\t1\t0\t0\t1\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\t0\t1\t0\n",
            "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t0\t0\t0\t0\t1\t0\t1\t1\t0\n",
            "1\t0\t0\t1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
            "1\t0\t1\t1\t1\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\n",
            "0\t0\t0\t1\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
            "0\t1\t1\t1\t1\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\t1\t1\t0\n",
            "0\t0\t1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t0\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\t0\n",
            "1\t0\t1\t1\t1\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\t1\t1\t1\n",
            "1\t0\t0\t1\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\n",
            "1\t1\t0\t1\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
            "1\t0\t0\t0\t1\t0\t0\t0\t1\t0\t0\t1\t1\t0\n",
            "1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t1\t1\t1\t1\n",
            "1\t0\t1\t0\t1\t1\t0\t0\t1\t0\t1\t1\t1\t0\n",
            "1\t1\t0\t0\t1\t0\t0\t1\t0\t0\t1\t0\t1\t0\n",
            "0\t1\t1\t0\t0\t1\t0\t1\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t1\t1\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\n",
            "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t1\t0\t0\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t0\t1\t1\n",
            "1\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t1\t1\t1\n",
            "0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t1\t0\t1\t1\n",
            "1\t0\t0\t0\t1\t1\t0\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\n",
            "0\t1\t0\t0\t1\t1\t1\t1\t1\t1\t1\t1\t1\t0\n",
            "1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t1\t0\t1\t1\t0\t0\t0\t1\t1\t1\t1\t1\t1\n",
            "1\t0\t1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t1\t1\t1\t0\t1\t1\t1\t0\t1\t1\n",
            "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t1\t1\t1\t0\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t0\t1\t0\t0\t1\t0\t1\t1\t0\t1\t1\t1\t0\n",
            "0\t0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t1\t0\n",
            "0\t0\t1\t1\t0\t1\t1\t1\t0\t1\t0\t0\t1\t0\n",
            "0\t0\t0\t0\t0\t1\t0\t1\t0\t0\t0\t0\t1\t0\n",
            "0\t1\t1\t0\t1\t0\t1\t0\t0\t1\t1\t0\t1\t0\n",
            "1\t1\t1\t0\t0\t0\t1\t0\t0\t0\t1\t1\t1\t0\n",
            "0\t1\t0\t1\t0\t0\t0\t0\t1\t1\t0\t1\t1\t0\n",
            "1\t1\t0\t0\t0\t0\t1\t0\t0\t0\t0\t1\t1\t0\n",
            "0\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\t0\t1\t0\n",
            "1\t1\t0\t1\t0\t0\t0\t0\t0\t1\t1\t0\t1\t1\n",
            "1\t1\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\t1\n",
            "0\t0\t0\t1\t0\t1\t1\t1\t0\t0\t1\t0\t1\t0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    with open(train_path, \"r\") as train_file:\n",
        "        contents = train_file.read()\n",
        "        print(contents)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {train_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X1\tX2\tX3\n",
            "1\t1\t1\t1\n",
            "0\t0\t1\t1\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "1\t1\t1\t1\n",
            "1\t0\t1\t0\n",
            "1\t0\t1\t0\n",
            "0\t0\t1\t1\n",
            "0\t1\t1\t0\n",
            "0\t1\t1\t0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    with open(test_path, \"r\") as test_file:\n",
        "        contents = test_file.read()\n",
        "        print(contents)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {test_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXHLhnhwiBR"
      },
      "source": [
        "### Build the Perceptron Model\n",
        "\n",
        "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cXAsP_lw3QwJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "\n",
        "# Corpus reader, all columns but the last one are coordinates;\n",
        "#   the last column is the label\n",
        "def read_data(file_name):\n",
        "    f = open(train_path, 'r')\n",
        "\n",
        "    data = []\n",
        "    # Discard header line\n",
        "    f.readline()\n",
        "    for instance in f.readlines():\n",
        "        if not re.search('\\t', instance): continue\n",
        "        instance = list(map(int, instance.strip().split('\\t')))\n",
        "        # Add a dummy input so that w0 becomes the bias\n",
        "        instance = [-1] + instance\n",
        "        data += [instance]\n",
        "    return data\n",
        "\n",
        "\n",
        "def dot_product(array1, array2):\n",
        "    #TODO: Return dot product of array 1 and array 2\n",
        "    \n",
        "    dp_result =  sum(x * y for x, y in zip(array1, array2))\n",
        "    \n",
        "    return dp_result\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    #TODO: Return output of sigmoid function on x\n",
        "    \n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# The output of the model, which for the perceptron is \n",
        "# the sigmoid function applied to the dot product of \n",
        "# the instance and the weights\n",
        "def output(weight, instance):\n",
        "     # Compute dot product of weights and instance\n",
        "    dp = dot_product(weight, instance)\n",
        "    # Apply sigmoid function to the summation\n",
        "    Output = sigmoid(dp)\n",
        "    return Output\n",
        "   \n",
        "\n",
        "# Predict the label of an instance; this is the definition of the perceptron\n",
        "# you should output 1 if the output is >= 0.5 else output 0\n",
        "def predict(weights, instance):\n",
        "    # Compute the perceptron output\n",
        "    prediction = output(weights, instance)\n",
        "    \n",
        "    # Output determination\n",
        "    \n",
        "    if prediction >= 0.5:\n",
        "            p = 1\n",
        "    else:\n",
        "            p = 0\n",
        "    return p\n",
        "\n",
        "\n",
        "# Accuracy = percent of correct predictions\n",
        "def get_accuracy(weights, instances):\n",
        "    # You do not to write code like this, but get used to it\n",
        "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
        "                   for instance in instances])\n",
        "    return correct * 100 / len(instances)\n",
        "\n",
        "\n",
        "# Train a perceptron with instances and hyperparameters:\n",
        "#       lr (learning rate) \n",
        "#       epochs\n",
        "# The implementation comes from the definition of the perceptron\n",
        "#\n",
        "# Training consists on fitting the parameters which are the weights\n",
        "# that's the only thing training is responsible to fit\n",
        "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
        "#\n",
        "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
        "# We are updating weights in the opposite direction of the gradient of the error,\n",
        "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
        "def train_perceptron(instances, lr, epochs):\n",
        "\n",
        "    #TODO: Initialize the weight vector for the perceptron to 0.\n",
        "    weights = [0] * (len(instances[0])-1)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        for instance in instances:\n",
        "            #TODO: summation of inputs and weights or the weighted sum\n",
        "            in_value = dot_product(weights, instance)\n",
        "            output = sigmoid(in_value)\n",
        "            error = instance[-1] - output\n",
        "            #TODO: Gradient descent-based weight update step. Updating the weights to decrease the error\n",
        "            for i in range(0, len(weights)):\n",
        "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adBZuMlAwiBT"
      },
      "source": [
        "## Run it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "50YvUza-BYQF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "lr = 0.005\n",
        "epochs = 5\n",
        "weights = train_perceptron(instances_tr, lr, epochs)\n",
        "accuracy = get_accuracy(weights, instances_te)\n",
        "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBXkvaiQMohX"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions. Include your implementation and the output for each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQ6BEk1CBlr"
      },
      "source": [
        "\n",
        "\n",
        "### Question 1\n",
        "\n",
        "In `train_perceptron(instances, lr, epochs)`, we have the following code:\n",
        "```\n",
        "in_value = dot_product(weights, instance)\n",
        "output = sigmoid(in_value)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "Why don't we have the following code snippet instead?\n",
        "```\n",
        "output = predict(weights, instance)\n",
        "error = instance[-1] - output\n",
        "```\n",
        "\n",
        "#### TODO Add your answer here (text only)\n",
        "\n",
        "\n",
        "\n",
        "### Because this is training not testing. The first code snippet would give the error to be able to decrease in the next iteration, but the second one will only give the output 0 or 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU3c3m6YL2rK"
      },
      "source": [
        "### Question 2\n",
        "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
        "\n",
        "```\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr = [0.005, 0.01, 0.05]              # learning rate\n",
        "```\n",
        "\n",
        "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
        "of your code.The output should look like the following:\n",
        "```\n",
        "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "[and so on for all the combinations]\n",
        "```\n",
        "You will get different results with different hyperparameters.\n",
        "\n",
        "#### TODO Add your answer here (code and output in the format above) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
        "lr_list = [0.005, 0.01, 0.05]              # learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n"
          ]
        }
      ],
      "source": [
        "print(int(len(instances_tr) * (75 / 100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:   5, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:   5, learning rate: 0.050; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:  10, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:  10, learning rate: 0.050; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:  20, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:  20, learning rate: 0.050; Accuracy (test, 400 instances): 71.8\n",
            "#tr:  20, epochs:  50, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  20, epochs:  50, learning rate: 0.010; Accuracy (test, 400 instances): 72.5\n",
            "#tr:  20, epochs:  50, learning rate: 0.050; Accuracy (test, 400 instances): 65.8\n",
            "#tr:  20, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 72.5\n",
            "#tr:  20, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 71.8\n",
            "#tr:  20, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 65.2\n",
            "#tr:  40, epochs:   5, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:   5, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:   5, learning rate: 0.050; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  10, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  10, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  10, learning rate: 0.050; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  20, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  20, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  20, learning rate: 0.050; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  50, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  50, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs:  50, learning rate: 0.050; Accuracy (test, 400 instances): 74.8\n",
            "#tr:  40, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr:  40, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 72.5\n",
            "#tr:  40, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 74.2\n",
            "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 400 instances): 73.5\n",
            "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 400 instances): 78.2\n",
            "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 400 instances): 72.8\n",
            "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 400 instances): 81.0\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 72.8\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 77.2\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 80.0\n",
            "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 400 instances): 77.0\n",
            "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 400 instances): 80.2\n",
            "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 400 instances): 72.8\n",
            "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 400 instances): 75.8\n",
            "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 400 instances): 79.5\n",
            "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 400 instances): 81.8\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 79.5\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 80.8\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 400 instances): 78.5\n",
            "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 400 instances): 82.0\n",
            "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 400 instances): 77.2\n",
            "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 400 instances): 81.5\n",
            "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 400 instances): 78.2\n",
            "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 400 instances): 82.0\n",
            "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 400 instances): 82.8\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 81.2\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 83.2\n",
            "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 400 instances): 79.0\n",
            "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 400 instances): 72.8\n",
            "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 400 instances): 81.8\n",
            "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 400 instances): 72.8\n",
            "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 400 instances): 78.0\n",
            "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 400 instances): 83.5\n",
            "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 400 instances): 79.2\n",
            "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 400 instances): 84.2\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 83.0\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 84.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "for tr in tr_percent:\n",
        "      \n",
        "      ins_size = len(instances_tr) * (tr / 100)\n",
        "      train_subset = instances_tr[:int(ins_size)] \n",
        "      \n",
        "      for epochs in num_epochs:\n",
        "            for lr in lr_list:\n",
        "                  weights = train_perceptron( train_subset, lr, epochs)\n",
        "                  accuracy = get_accuracy(weights, instances_te)\n",
        "                  print(f\"#tr: {len( train_subset):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "                        f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "G-VKJOUu2BTp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 72.5\n",
            "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 72.2\n",
            "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 72.8\n",
            "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 79.5\n",
            "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 71.8\n",
            "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 72.5\n",
            "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 77.2\n",
            "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 80.8\n",
            "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 81.2\n",
            "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 400 instances): 83.0\n",
            "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 65.2\n",
            "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 74.2\n",
            "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 80.0\n",
            "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 82.2\n",
            "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 83.2\n",
            "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 400 instances): 84.0\n"
          ]
        }
      ],
      "source": [
        "instances_tr = read_data(\"train.dat\")\n",
        "instances_te = read_data(\"test.dat\")\n",
        "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
        "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
        "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
        "\n",
        "for lr in lr_array:\n",
        "  for tr_size in tr_percent:\n",
        "    for epochs in num_epochs:\n",
        "      size =  round(len(instances_tr)*tr_size/100)\n",
        "      pre_instances = instances_tr[0:size]\n",
        "      weights = train_perceptron(pre_instances, lr, epochs)\n",
        "      accuracy = get_accuracy(weights, instances_te)\n",
        "    print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
        "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB9MtwML24O"
      },
      "source": [
        "### Question 3\n",
        "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
        "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
        "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
        "   ```\n",
        "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
        "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
        "```\n",
        "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
        "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
        "\n",
        "#### TODO: Add your answer here (code and text)\n",
        "\n",
        "A. \n",
        "\n",
        "Training with 100% of the dataset does not always result in the highest accuracy. In some cases, using 50% or 75% of the dataset provides similar or, sometimes, even better accuracy. This suggests that after a certain amount of training data, additional examples may contribute in decreasing returns.\n",
        "\n",
        "Sometimes, it's also the overfitting. \n",
        "\n",
        "B.\n",
        "\n",
        "Maybe because of the different learning rates, where it may causing the model to converge too slowly or not reach an optimal state. Also, sometimes additional data may produce noise of misleading or contains many outliers. \n",
        "\n",
        "\n",
        "C. \n",
        "\n",
        "Yes, it is possible to achieve higher accuracy by :\n",
        "\n",
        "- Increasing the Number of Epochs.\n",
        "\n",
        "- Using an better and well-suited Learning Rate. (not too slow or too fast)\n",
        "\n",
        "\n",
        "- Using a Different Activation Functions. (sometimes, activation function is what makes difference in different problems)\n",
        "\n",
        "\n",
        "\n",
        "D. \n",
        "\n",
        "No, sometimes it may produce overfitting and may also diminish returns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rA_Kp3wiBX"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW2_The_Perceptron.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
